"""
ë²¡í„° DB ì—…ë°ì´íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (ì›¹ í¬ë¡¤ë§ í¬í•¨)

ì—­í• :
1. ì›¹ì—ì„œ ìµœì‹  ì‚¬ê¸° ë‰´ìŠ¤ í¬ë¡¤ë§
2. ê³ ì† ì„ë² ë”© (ë°°ì¹˜ ì²˜ë¦¬)
3. ChromaDBì— ì¶”ê°€
"""

import hashlib
import json
import sys
from pathlib import Path

PROJECT_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(PROJECT_ROOT))

from scripts.web_crawler import ScamNewsCrawler
from infrastructure.vector_store.scam_repository import FastScamRepository
from datetime import datetime

DEFAULT_BATCH_SIZE = 50



def load_json_files(data_dir: str = "data") -> list:
    """
    data/ í´ë”ì˜ JSON íŒŒì¼ ë¡œë“œ

    list[dict] í˜•íƒœë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    """
    all_records = []
    data_path = Path(data_dir)
    if not data_path.exists():
        return all_records
    for json_file in data_path.glob("*.json"):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if isinstance(data, list):
                # list[dict] ê²€ì¦
                valid = [d for d in data if isinstance(d, dict) and d.get('title')]
                all_records.extend(valid)
                print(f"  ğŸ“„ {json_file.name}: {len(valid)}ê°œ ë¡œë“œ")
            else:
                print(f"  âš ï¸ {json_file.name}: list í˜•íƒœê°€ ì•„ë‹˜, ìŠ¤í‚µ")
        except Exception as e:
            print(f"  âš ï¸ {json_file.name} ë¡œë“œ ì‹¤íŒ¨: {e}")
    return all_records

def load_csv_files(data_dir: str = "data") -> list:
    """
    data/ í´ë”ì˜ CSV íŒŒì¼ì„ pandasë¡œ ì½ì–´ records(dict list)ë¡œ ë³€í™˜
    """
    all_records = []
    data_path = Path(data_dir)
    if not data_path.exists():
        return all_records

    try:
        import pandas as pd
    except ImportError:
        print("  âš ï¸ pandas ë¯¸ì„¤ì¹˜, CSV ë¡œë“œ ìŠ¤í‚µ")
        return all_records

    for csv_file in data_path.glob("*.csv"):
        try:
            df = pd.read_csv(csv_file, encoding='utf-8')
            records = df.to_dict('records')
            # ìµœì†Œí•œ title í•„ë“œê°€ ìˆëŠ” ë ˆì½”ë“œë§Œ
            valid = [r for r in records if r.get('title')]
            all_records.extend(valid)
            print(f"  ğŸ“„ {csv_file.name}: {len(valid)}ê°œ ë¡œë“œ")
        except Exception as e:
            print(f"  âš ï¸ {csv_file.name} ë¡œë“œ ì‹¤íŒ¨: {e}")

    return all_records


def update_vectorstore_with_web_data(batch_size: int = DEFAULT_BATCH_SIZE) -> bool:
    """
    ì›¹ í¬ë¡¤ë§ ë°ì´í„°ë¡œ ë²¡í„° DB ì—…ë°ì´íŠ¸
    """
    print("\n" + "="*60)
    print("ğŸ•·ï¸ ì›¹ í¬ë¡¤ë§ + ë²¡í„° DB ì—…ë°ì´íŠ¸")
    print("="*60)
    
    # Step 1: ì›¹ í¬ë¡¤ë§
    print("\n[Step 1/3] ì›¹ í¬ë¡¤ë§ ì¤‘...")
    crawler = ScamNewsCrawler()

    news_list = crawler.crawl_multiple_keywords(
        keywords=["ë³´ì´ìŠ¤í”¼ì‹±", "ë©”ì‹ ì €í”¼ì‹±", "ìŠ¤ë¯¸ì‹±", "ëŒ€ì¶œì‚¬ê¸°", "íˆ¬ìì‚¬ê¸°"],
        max_per_keyword=10
    )
    print(f"âœ… ì´ {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")

    # Step 2: data/ í´ë”ì˜ JSON, CSV ë¡œë“œ
    print("\n[Step 2/4] data/ í´ë” íŒŒì¼ ë¡œë“œ ì¤‘...")
    json_records = load_json_files("data")
    csv_records = load_csv_files("data")

    print(f"  JSON: {len(json_records)}ê°œ / CSV: {len(csv_records)}ê°œ")

    # ë¡œì»¬ íŒŒì¼ ë ˆì½”ë“œì— ê¸°ë³¸ í•„ë“œ ë³´ì™„
    for record in json_records + csv_records:
        record.setdefault('source', 'local_file')
        record.setdefault('keyword', '')
        record.setdefault('crawled_at', datetime.now().isoformat())
        record.setdefault('description', '')
        record.setdefault('link', '')
        record.setdefault('press', '')
        record.setdefault('date', '')

    # Step 3: í•©ì‚° + ì¤‘ë³µ ì œê±°
    print("\n[Step 3/4] ë°ì´í„° í•©ì‚° + ì¤‘ë³µ ì œê±°...")
    combined = news_list + json_records +csv_records
    combined = crawler.dedup_by_link(combined)
    print(f"  í•©ì‚° í›„: {len(combined)}ê°œ")
    
    # Step 4: Document ë³€í™˜
    documents = crawler.convert_to_documents(combined)

    print(f"âœ… {len(documents)}ê°œ Document ìƒì„± ì™„ë£Œ")
    
    # Step 3: ë²¡í„° DBì— ì¶”ê°€ (ë°°ì¹˜ ì²˜ë¦¬)
    print("\n[Step 3/3] ë²¡í„° DB ì—…ë°ì´íŠ¸ ì¤‘...(ë°°ì¹˜: {batch_size})")

    try:
        repo = FastScamRepository(batch_size=batch_size)
        
        # ë°°ì¹˜ ì¶”ê°€ (ê³ ì†)
        repo.add_documents_batch(documents, batch_size=batch_size)
        
        print(f"âœ… ë²¡í„° DB ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
        print(f"   í˜„ì¬ ì´ ë¬¸ì„œ ìˆ˜: {repo.collection.count()}")
        
    except Exception as e:
        print(f"âŒ ë²¡í„° DB ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        return False
    
    # Step 4: ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ“Š ì—…ë°ì´íŠ¸ ìš”ì•½")
    print("="*60)
    print(f"  í¬ë¡¤ë§ ë‰´ìŠ¤: {len(news_list)}ê°œ")
    print(f"  JSON íŒŒì¼: {len(json_records)}ê°œ")
    print(f"  CSV íŒŒì¼: {len(csv_records)}ê°œ")
    print(f"  í•©ì‚°(dedup): {len(combined)}ê°œ")
    print(f"  ìƒì„± Document: {len(documents)}ê°œ")
    print(f"  DB ì´ ë¬¸ì„œ: {repo.collection.count()}ê°œ")
    print(f"  ì—…ë°ì´íŠ¸ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)
    
    return True


if __name__ == "__main__":
    success = update_vectorstore_with_web_data()
    sys.exit(0 if success else 1)