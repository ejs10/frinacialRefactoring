"""
ìë™ í¬ë¡¤ë§ + ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ì—­í• :
1. ì›¹ í¬ë¡¤ë§
2. ìë™ ë¶„ì„
3. ê²°ê³¼ ì €ì¥
"""

import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

PRPJECT_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(PRPJECT_ROOT))

from agent.graph import get_graph
from agent.state import AgentState
from scripts.web_crawler import ScamNewsCrawler

async def analyze_news(graph, news_item: Dict[str, Any], index: int) -> Optional[Dict[str, Any]]:
    """ë‰´ìŠ¤ ë¶„ì„"""
    print(f"\n[{index}] ë¶„ì„ ì¤‘: {news_item['title'][:50]}...")

    initial_state = {
        "message": news_item['title'] + "\n" + news_item.get('description', ''),
        "sender": None,
        "scam_type": None,
        "confidence": None,
        "similar_cases": [],
        "matched_patterns": [],
        "risk_level": None,
        "risk_score": None,
        "risk_factors": [],
        "is_scam": None,
        "analysis": None,
        "recommendations": None,
        "processing_time": None,
        "completed": False,
    }

    # AI ì‹¤í–‰
    try:
        result = await graph.ainvoke(initial_state)
        
        print(f"  â†’ ì‚¬ê¸° ìœ í˜•: {result.get('scam_type')}")
        print(f"  â†’ ìœ„í—˜ë„: {result.get('risk_level')} ({result.get('risk_score')}ì )")
        
        return {
            "news": news_item,
            "analysis": {
                "is_scam": result.get("is_scam"),
                "scam_type": result.get("scam_type"),
                "risk_level": result.get("risk_level"),
                "risk_score": result.get("risk_score"),
            }
        }
    except Exception as e:
        print(f"  âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None
    
async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("ğŸ¤– ìë™ í¬ë¡¤ë§ + ë¶„ì„ ì‹œìŠ¤í…œ")
    print("="*60)
    
    # Step 1: í¬ë¡¤ë§
    print("\n[Step 1/3] ì›¹ í¬ë¡¤ë§ ì¤‘...")
    crawler = ScamNewsCrawler()
    news_list = crawler.crawl_multiple_keywords(
        keywords=["ë³´ì´ìŠ¤í”¼ì‹±", "ëŒ€ì¶œì‚¬ê¸°"],
        max_per_keyword=5
    )
    print(f"âœ… {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
    
    # Step 2: AI ì—ì´ì „íŠ¸ ë¡œë“œ
    print("\n[Step 2/3] AI ì—ì´ì „íŠ¸ ë¡œë“œ ì¤‘...")
    graph = get_graph()
    print("âœ… ì—ì´ì „íŠ¸ ì¤€ë¹„ ì™„ë£Œ")
    
    # Step 3: ë¶„ì„
    print("\n[Step 3/3] ë‰´ìŠ¤ ë¶„ì„ ì¤‘...")
    results: List[Dict[str, Any]] = []

    for idx, news in enumerate(news_list[:10], 1):
        result = await analyze_news(graph, news, idx)
        if result:
            results.append(result)
        await asyncio.sleep(1)
    
    output_dir = PRPJECT_ROOT / "data" / "analysis_results"
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"auto_analysis_{timestamp}.json"

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_file}")
    
    # ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ“Š ë¶„ì„ ìš”ì•½")
    print("="*60)
    print(f"  í¬ë¡¤ë§: {len(news_list)}ê°œ")
    print(f"  ë¶„ì„ ì„±ê³µ: {len(results)}ê°œ")
    
    scam_count = sum(1 for r in results if r['analysis'].get('is_scam'))
    print(f"  ì‚¬ê¸° íŒì •: {scam_count}ê°œ")
    print("="*60)

if __name__ == "__main__":
    asyncio.run(main())