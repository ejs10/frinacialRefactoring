"""
ì›¹ í¬ë¡¤ë§ ë…¸ë“œ

ì—­í• :
- ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ìµœì‹  ì‚¬ê¸° ì‚¬ë¡€ í¬ë¡¤ë§
- ê¸ˆê°ì›, ê²½ì°°ì²­ ë“± ê³µì‹ ì •ë³´ ìˆ˜ì§‘
- í¬ë¡¤ë§ ë°ì´í„°ë¥¼ Documentë¡œ ë³€í™˜
"""

import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional
import time
from datetime import datetime
from langchain_core.documents import Document

class ScamNewsCrawler:
    """ì‚¬ê¸° ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    def crawl_naver_news(
        self,
        keyword: str = "ë³´ì´ìŠ¤í”¼ì‹±",
        max_count: int = 10
    ) -> List[Dict[str, Any]]:
        """
        ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
        
        Returns:
            ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\nğŸ•·ï¸ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘... (í‚¤ì›Œë“œ: {keyword})")

        url = f"https://search.naver.com/search.naver?where=news&query={keyword}"

        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text,'html.parser')

            news_list: List[Dict[str,Any]] = []
            # ë‰´ìŠ¤ ì•„ì´í…œ ì¶”ì¶œ
            for idx, item in enumerate(soup.select('.news_area'), 1):
                if idx > max_count:
                    break
                
                try:
                    # ì œëª©
                    title_elem = item.select_one('.news_tit')
                    title = title_elem.get_text().strip() if title_elem else ""
                    
                    # ìš”ì•½
                    desc_elem = item.select_one('.news_dsc')
                    description = desc_elem.get_text().strip() if desc_elem else ""
                    
                    # ë§í¬
                    link = title_elem.get('href') if title_elem else ""
                    
                    # ì–¸ë¡ ì‚¬
                    press_elem = item.select_one('.info.press')
                    press = press_elem.get_text().strip() if press_elem else ""
                    
                    # ë‚ ì§œ
                    date_elem = item.select_one('.info')
                    date = date_elem.get_text().strip() if date_elem else ""
                    
                    if title:
                        news_list.append({
                            'title': title,
                            'description': description,
                            'link': link,
                            'press': press,
                            'date': date,
                            'source': 'naver_news',
                            'keyword': keyword,
                            'crawled_at': datetime.now().isoformat()
                        })
                
                except Exception as e:
                    print(f"  âš ï¸ ë‰´ìŠ¤ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue
            
            print(f"  âœ… {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
            return news_list
        
        except Exception as e:
            print(f"  âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
        
    def crawl_multiple_keywords(
        self,
        keywords: Optional[List[str]] = None,
        max_per_keyword: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ í¬ë¡¤ë§
        
        Args:
            keywords: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            max_per_keyword: í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê°œìˆ˜
        
        Returns:
            ì „ì²´ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        if keywords is None:
            keywords = [
                "ë³´ì´ìŠ¤í”¼ì‹±",
                "ë©”ì‹ ì €í”¼ì‹±",
                "ìŠ¤ë¯¸ì‹±",
                "ëŒ€ì¶œì‚¬ê¸°",
                "íˆ¬ìì‚¬ê¸°",
                "ê¸ˆìœµì‚¬ê¸°"
            ]
        all_news = []

        for keyword in keywords:
            news = self.crawl_naver_news(keyword, max_per_keyword)
            all_news.extend(news)
            time.sleep(1)

        return all_news
    def convert_to_documents(self, news_list: List[Dict[str,Any]]) -> List[Document]:
        """
        ë‰´ìŠ¤ë¥¼ Documentë¡œ ë³€í™˜
        
        Args:
            news_list: í¬ë¡¤ë§í•œ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            Document ë¦¬ìŠ¤íŠ¸
        """
        documents = []

        for news in news_list:
            content = f"ì œëª©: {news['title']}\n"
            if news.get('description'):
                content += f"ë‚´ìš©: {news['description']}\n"
            doc = Document(
                page_content=content,
                metadata={
                    'source': news['source'],
                    'keyword': news['keyword'],
                    'press': news.get('press', ''),
                    'date': news.get('date', ''),
                    'link': news.get('link', ''),
                    'crawled_at': news['crawled_at'],
                    'scam_type': news['keyword'],  # í‚¤ì›Œë“œë¥¼ ì‚¬ê¸° ìœ í˜•ìœ¼ë¡œ ì‚¬ìš©
                    'origin': 'web_crawling'
                }
            )
            documents.append(doc)
        return documents

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    crawler = ScamNewsCrawler()
    
    # í¬ë¡¤ë§
    news = crawler.crawl_multiple_keywords(max_per_keyword=3)
    
    # Document ë³€í™˜
    documents = crawler.convert_to_documents(news)
    
    print(f"\nì´ {len(documents)}ê°œ Document ìƒì„±")
    
    # ì²« ë²ˆì§¸ ë¬¸ì„œ ì¶œë ¥
    if documents:
        print(f"\nì˜ˆì‹œ:")
        print(documents[0].page_content[:200])