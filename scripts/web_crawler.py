"""
ì›¹ í¬ë¡¤ë§ ë…¸ë“œ

ì—­í• :
- ë„¤ì´ë²„ ë‰´ìŠ¤ì—ì„œ ìµœì‹  ì‚¬ê¸° ì‚¬ë¡€ í¬ë¡¤ë§
- ê¸ˆê°ì›, ê²½ì°°ì²­ ë“± ê³µì‹ ì •ë³´ ìˆ˜ì§‘
- í¬ë¡¤ë§ ë°ì´í„°ë¥¼ Documentë¡œ ë³€í™˜
"""

import hashlib
import json
import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional
import time
from datetime import datetime
from langchain_core.documents import Document

class ScamNewsCrawler:
    """ì‚¬ê¸° ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    def crawl_naver_news(
        self,
        keyword: str = "ë³´ì´ìŠ¤í”¼ì‹±",
        max_count: int = 10
    ) -> List[Dict[str, Any]]:
        """
        ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
        
        Returns:
            ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\nğŸ•·ï¸ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘... (í‚¤ì›Œë“œ: {keyword})")

        url = f"https://search.naver.com/search.naver?where=news&query={keyword}"

        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text,'html.parser')

            news_list: List[Dict[str,Any]] = []
            # ë‰´ìŠ¤ ì•„ì´í…œ ì¶”ì¶œ
            for idx, item in enumerate(soup.select('.news_area'), 1):
                if idx > max_count:
                    break
                
                try:
                    # ì œëª©
                    title_elem = item.select_one('.news_tit')
                    title = title_elem.get_text().strip() if title_elem else ""
                    
                    # ìš”ì•½
                    desc_elem = item.select_one('.news_dsc')
                    description = desc_elem.get_text().strip() if desc_elem else ""
                    
                    # ë§í¬
                    link = title_elem.get('href') if title_elem else ""
                    
                    # ì–¸ë¡ ì‚¬
                    press_elem = item.select_one('.info.press')
                    press = press_elem.get_text().strip() if press_elem else ""
                    
                    # ë‚ ì§œ
                    date_elem = item.select_one('.info')
                    date = date_elem.get_text().strip() if date_elem else ""
                    
                    if title:
                        news_list.append({
                            'title': title,
                            'description': description,
                            'link': link,
                            'press': press,
                            'date': date,
                            'source': 'naver_news',
                            'keyword': keyword,
                            'crawled_at': datetime.now().isoformat()
                        })
                
                except Exception as e:
                    print(f"  âš ï¸ ë‰´ìŠ¤ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue
            
            print(f"  âœ… {len(news_list)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
            return news_list
        
        except Exception as e:
            print(f"  âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
        
    def crawl_fss_alerts(
        self,
        max_count: int = 10
    ) -> List[Dict[str, Any]]:
        """
        ê¸ˆìœµê°ë…ì› ì†Œë¹„ìê²½ë³´ í¬ë¡¤ë§

        Args:
            max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜

        Returns:
            ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ê¸°ì¡´ dict ìŠ¤í‚¤ë§ˆ ë™ì¼)
        """

        print(f"\nğŸ›ï¸ ê¸ˆìœµê°ë…ì› ì†Œë¹„ìê²½ë³´ í¬ë¡¤ë§ ì¤‘...")

        url = "https://www.fss.or.kr/fss/bbs/B0000188/list.do?menuNo=200218"

        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            results: List[Dict[str, Any]] = []

            for idx, row in enumerate(soup.select('table tbody tr'), 1):
                if idx > max_count:
                    break
                try:
                    title_elem = row.select_one('td.tit a, td a')
                    if not title_elem:
                        continue
                    title = title_elem.get_text().strip()
                    link_href = title_elem.get('href', '')
                    if link_href and not link_href.startswith('http'):
                        link_href = "https://www.fss.or.kr" + link_href

                    date_elem = row.select_one('td.date, td:nth-of-type(4)')
                    date = date_elem.get_text().strip() if date_elem else ""

                    if title:
                        results.append({
                            'title': title,
                            'description': '',
                            'link': link_href,
                            'press': 'ê¸ˆìœµê°ë…ì›',
                            'date': date,
                            'source': 'fss_alert',
                            'keyword': 'ê¸ˆìœµì‚¬ê¸°',
                            'crawled_at': datetime.now().isoformat()
                        })
                except Exception as e:
                    print(f"  âš ï¸ FSS íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue
            print(f"  âœ… ê¸ˆê°ì› {len(results)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            return results

        except Exception as e:
            print(f"  âŒ ê¸ˆê°ì› í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []

    def crawl_police_cyber(
        self,
        max_count: int = 10
    ) -> List[Dict[str, Any]]:
        """
        ê²½ì°°ì²­ ì‚¬ì´ë²„ìˆ˜ì‚¬êµ­ ë³´ì´ìŠ¤í”¼ì‹± ê³µì§€ í¬ë¡¤ë§

        Args:
            max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜

        Returns:
            ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ (ê¸°ì¡´ dict ìŠ¤í‚¤ë§ˆ ë™ì¼)
        """
        print(f"\nğŸš” ê²½ì°°ì²­ ì‚¬ì´ë²„ìˆ˜ì‚¬êµ­ í¬ë¡¤ë§ ì¤‘...")

        url = "https://ecrm.police.go.kr/minwon/bbs/B0000060/list.do"

        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            results: List[Dict[str, Any]] = []

            for idx, row in enumerate(soup.select('table tbody tr, .board_list li'), 1):
                if idx > max_count:
                    break
                try:
                    title_elem = row.select_one('a')
                    if not title_elem:
                        continue

                    title = title_elem.get_text().strip()
                    link_href = title_elem.get('href', '')
                    if link_href and not link_href.startswith('http'):
                        link_href = "https://ecrm.police.go.kr" + link_href

                    date_elem = row.select_one('td.date, td:nth-of-type(4), .date')
                    date = date_elem.get_text().strip() if date_elem else ""

                    if title:
                        results.append({
                            'title': title,
                            'description': '',
                            'link': link_href,
                            'press': 'ê²½ì°°ì²­',
                            'date': date,
                            'source': 'police_cyber',
                            'keyword': 'ë³´ì´ìŠ¤í”¼ì‹±',
                            'crawled_at': datetime.now().isoformat()
                        })
                except Exception as e:
                    print(f"  âš ï¸ ê²½ì°°ì²­ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue

            print(f"  âœ… ê²½ì°°ì²­ {len(results)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            return results

        except Exception as e:
            print(f"  âŒ ê²½ì°°ì²­ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
    
    @staticmethod
    def dedup_by_link(news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """link ê¸°ì¤€ ì¤‘ë³µ ì œê±° (link ì—†ìœ¼ë©´ title í•´ì‹œë¡œ ëŒ€ì²´)"""
        seen = set()
        deduped = []
        for item in news_list:
            key = item.get('link') or hashlib.md5(
                item.get('title', '').encode()
            ).hexdigest()
            if key not in seen:
                seen.add(key)
                deduped.append(item)
        removed = len(news_list) - len(deduped)
        if removed > 0:
            print(f"  ğŸ”„ ì¤‘ë³µ ì œê±°: {removed}ê°œ ì œê±° â†’ {len(deduped)}ê°œ ìœ ì§€")
        return deduped
        
    def crawl_multiple_keywords(
        self,
        keywords: Optional[List[str]] = None,
        max_per_keyword: int = 5
    ) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ í¬ë¡¤ë§
        
        Args:
            keywords: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            max_per_keyword: í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê°œìˆ˜
        
        Returns:
            ì „ì²´ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        if keywords is None:
            keywords = [
                "ë³´ì´ìŠ¤í”¼ì‹±",
                "ë©”ì‹ ì €í”¼ì‹±",
                "ìŠ¤ë¯¸ì‹±",
                "ëŒ€ì¶œì‚¬ê¸°",
                "íˆ¬ìì‚¬ê¸°",
                "ê¸ˆìœµì‚¬ê¸°"
            ]
        all_news = []

        for keyword in keywords:
            news = self.crawl_naver_news(keyword, max_per_keyword)
            all_news.extend(news)
            time.sleep(1)

        return all_news
    def convert_to_documents(self, news_list: List[Dict[str,Any]]) -> List[Document]:
        """
        ë‰´ìŠ¤ë¥¼ Documentë¡œ ë³€í™˜
        
        Args:
            news_list: í¬ë¡¤ë§í•œ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            Document ë¦¬ìŠ¤íŠ¸
        """
        documents = []

        for news in news_list:
            content = f"ì œëª©: {news['title']}\n"
            if news.get('description'):
                content += f"ë‚´ìš©: {news['description']}\n"
            doc = Document(
                page_content=content,
                metadata={
                    'source': news['source'],
                    'keyword': news['keyword'],
                    'press': news.get('press', ''),
                    'date': news.get('date', ''),
                    'link': news.get('link', ''),
                    'crawled_at': news['crawled_at'],
                    'scam_type': news['keyword'],  # í‚¤ì›Œë“œë¥¼ ì‚¬ê¸° ìœ í˜•ìœ¼ë¡œ ì‚¬ìš©
                    'origin': 'web_crawling'
                }
            )
            documents.append(doc)
        return documents

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    crawler = ScamNewsCrawler()
    
    # í¬ë¡¤ë§
    news = crawler.crawl_multiple_keywords(max_per_keyword=3)
    
    # Document ë³€í™˜
    documents = crawler.convert_to_documents(news)
    
    print(f"\nì´ {len(documents)}ê°œ Document ìƒì„±")
    
    # ì²« ë²ˆì§¸ ë¬¸ì„œ ì¶œë ¥
    if documents:
        print(f"\nì˜ˆì‹œ:")
        print(documents[0].page_content[:200])