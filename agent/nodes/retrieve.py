"""
ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ë° íŒ¨í„´ ë§¤ì¹­ ë…¸ë“œ (RAG)

ì—­í• :
- ChromaDBì—ì„œ ìœ ì‚¬ ì‚¬ê¸° ì‚¬ë¡€ ê²€ìƒ‰ (ë²¡í„° ê²€ìƒ‰)
- ì‹¤ì‹œê°„ íŒ¨í„´ ë¶„ì„ (scam_patterns.json)
- ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì†ë„ ìµœì í™”

ê¸°ì¡´ scam_defense.pyì˜ ë¡œì§ í™œìš©
"""

import json
import asyncio
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor
import hashlib

from agent.state import AgentState
from langchain.schema import Document

_BASE_DIR = Path(__file__).resolve().parents[2]
_PATTERN_FILE = _BASE_DIR / "data" / "scam_defense" / "scam_patterns.json"
_PATTERN_CACHE: Optional[Dict] = None
_QUERY_CACHE: Dict[str, Tuple] = {}
_CACHE_SIZE_LIMIT = 100

_DANGER_LEVEL_ORDER = {
    "ë§¤ìš°ë†’ìŒ": 4,
    "ë†’ìŒ": 3,
    "ì¤‘ê°„": 2,
    "ë‚®ìŒ": 1,
    "ì •ë³´": 0,
}


# ========== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ========== #
@lru_cache(maxsize=1)
def _load_patterns() -> Dict:
    """íŒ¨í„´ JSON ë¡œë“œ(ì‹±ê¸€í†¤)"""
    global _PATTERN_CACHE

    if _PATTERN_CACHE is not None:
        return _PATTERN_CACHE

    try:
        if _PATTERN_FILE.exists():
            with open(_PATTERN_FILE, "r", encoding="utf-8") as f:
                _PATTERN_CACHE = json.load(f)
                print(
                    f"  âœ“ íŒ¨í„´ ë¡œë“œ: {len(_PATTERN_CACHE.get('financial_scams', []))}ê°œ"
                )
        else:
            print(f"  âš ï¸ íŒ¨í„´ íŒŒì¼ ì—†ìŒ: {_PATTERN_FILE}")
            _PATTERN_CACHE = {}
    except Exception as e:
        print(f"  âš ï¸ íŒ¨í„´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        _PATTERN_CACHE = {}
    return _PATTERN_CACHE


@lru_cache(maxsize=2048)
def _digits_only(value: Optional[str]) -> str:
    """ìˆ«ìë§Œ ì¶”ì¶œ (ìºì‹œ)"""
    return "".join(ch for ch in (value or "") if ch.isdigit())


# ì¿¼ë¦¬ í•´ë¦¬ ìƒì„±
def _hash_query(query: str, sender: Optional[str]) -> str:
    """ì¿¼ë¦¬ í•´ì‹œ ìƒì„± (ìºì‹œ í‚¤)."""
    key = f"{query}|{sender or ''}"
    return hashlib.md5(key.encode()).hexdigest()


def _clean_cache():
    """ìºì‹œ í¬ê¸° ì œí•œ."""
    global _QUERY_CACHE
    if len(_QUERY_CACHE) > _CACHE_SIZE_LIMIT:
        # ê°€ì¥ ì˜¤ë˜ëœ ì ˆë°˜ ì œê±°
        keys = list(_QUERY_CACHE.keys())
        for key in keys[: _CACHE_SIZE_LIMIT // 2]:
            _QUERY_CACHE.pop(key, None)


# ========== ì‹¤ì‹œê°„ íŒ¨í„´ ë¶„ì„ (ê¸°ì¡´ë¡œì§ - ë³µë¶™) ========== #
def analyze_realtime_patterns(
    self, query: str, sender: Optional[str] = None
) -> Tuple[List[Document], Dict]:
    """ì‹¤ì‹œê°„ íŒ¨í„´ ë¶„ì„ (ê¸°ì¡´ scam_defense.py ë¡œì§)

    Args:
        query: ë¶„ì„í•  ë©”ì‹œì§€
        sender: ë°œì‹ ì ì •ë³´

    Returns:
        (íŒ¨í„´ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸, ë¶„ì„ ê²°ê³¼)"""
    # ìºì‹œ í™•ì¸
    cache_key = _hash_query(query, sender)
    if cache_key in _QUERY_CACHE:
        return _QUERY_CACHE[cache_key]

    # íŒ¨í„´ë¡œë“œ
    dataset = _load_patterns()
    if not dataset:
        result = ([], {})
        _QUERY_CACHE[cache_key] = result
        return result

    query_lower = query.strip().lower()
    if not query_lower:
        result = ([], {})
        _QUERY_CACHE[cache_key] = result
        return result

    sender_lower = (sender or "").strip().lower()
    query_digits = _digits_only(query)
    sender_digits = _digits_only(sender)

    pattern_docs = []
    scam_matches = []
    highest_score = -1
    highest_level = None

    # 1. ì‚¬ê¸° íŒ¨í„´ ë§¤ì¹­
    for scam in dataset.get("financial_scams", [])[:20]:  # ìµœëŒ€ 20ê°œë§Œ
        patterns = [
            p for p in scam.get("patterns", []) if p and p.lower() in query_lower
        ]
        # ë°œì‹ ì íŒ¨í„´ë§¤ì¹­
        sender_patterns = [
            p
            for p in scam.get("sender_patterns", [])
            if p
            and (
                p.lower() in query_lower or (sender_lower and p.lower() in sender_lower)
            )
        ]

        if not patterns and not sender_patterns:
            continue

        scam_type = scam.get("type", "ì•Œ ìˆ˜ ì—†ìŒ")
        danger = scam.get("danger_level", "ì •ë³´")
        score = _DANGER_LEVEL_ORDER.get(danger, -1)

        if score > highest_score:
            highest_score = score
            highest_level = danger

        # ê°„ì†Œí™”ëœ ë¬¸ì„œ ìƒì„±
        content = f"ìœ í˜•: {scam_type} | ìœ„í—˜ë„: {danger}"
        if patterns:
            content += f"\níŒ¨í„´: {', '.join(patterns[:3])}"  # ìµœëŒ€ 3ê°œ

        pattern_docs.append(
            Document(
                page_content=content,
                metadata={
                    "source": "ì‹¤ì‹œê°„íŒ¨í„´",
                    "scam_type": scam_type,
                    "danger_level": danger,
                    "origin": "pattern_matchinng",
                },
            )
        )

        scam_matches.append(
            {
                "scam_type": scam_type,
                "danger_level": danger,
                "matched_patterns": patterns[:3],  # ì¶•ì†Œ
            }
        )

    # 2. í‚¤ì›Œë“œ ë§¤ì¹­ (ê°„ì†Œí™”)
    keyword_matches = {}
    for risk_level, keywords in (dataset.get("keywords") or {}).items():
        hits = [k for k in keywords[:10] if k and k.lower() in query_lower]  # ìµœëŒ€ 10ê°œ
        if hits:
            keyword_matches[risk_level] = hits[:3]  # ì¶•ì†Œ
            score = _DANGER_LEVEL_ORDER.get(risk_level, -1)
            if score > highest_score:
                highest_score = score
                highest_level = risk_level

    # 3. ê³µì‹ ì—°ë½ì²˜ (ê°„ì†Œí™”)
    legitimate_matches = []
    for org, phone in list((dataset.get("legitimate_contacts") or {}).items())[
        :5
    ]:  # ìµœëŒ€ 5ê°œ
        norm_phone = _digits_only(phone)
        if (org and org.lower() in query_lower) or (
            norm_phone and (norm_phone in query_digits or norm_phone in sender_digits)
        ):
            legitimate_matches.append({"organization": org, "phone": phone})
            pattern_docs.append(
                Document(
                    page_content=f"{org} ê³µì‹: {phone}",
                    metadata={"source": "ê³µì‹ì—°ë½ì²˜", "origin": "web_search"},
                )
            )

    # ê²°ê³¼ ìš”ì•½
    pattern_analysis = {
        "query": query.strip()[:100],  # ì¶•ì†Œ
        "sender": (sender or "").strip()[:50],
        "risk_summary": {
            "highest_level": highest_level,
            "score": highest_score,
            "is_high_risk": highest_score >= 3,
        },
        "scam_matches": scam_matches[:5],  # ìµœëŒ€ 5ê°œ
        "keyword_matches": keyword_matches,
        "legitimate_contacts": legitimate_matches[:3],  # ìµœëŒ€ 3ê°œ
    }

    result = (pattern_docs[:5], pattern_analysis)  # ìµœëŒ€ 5ê°œ ë¬¸ì„œ

    # ìºì‹œ ì €ì¥
    _QUERY_CACHE[cache_key] = result
    _clean_cache()

    return result


# ========== RAG ê²€ìƒ‰ ========== #
def search_vector_store(query: str, k: int = 5) -> List[Document]:
    """
    ChromaDBì—ì„œ ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜

    Returns:
        ìœ ì‚¬ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    try:
        from infrastructure.vector_store.scam_repository import ChromaScamRepository
        from app.config import settings

        # ë¦¬í¬ì§€í† ë¦¬ ìƒì„±
        repo = ChromaScamRepository(
            persist_directory=settings.CHROMA_PATH,
            embedding_api_key=settings.UPSTAGE_API_KEY,
        )

        results = repo.search(query=query, k=k)
        return results
    except Exception as e:
        print(f"  âš ï¸ ChromaDB ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []


async def retrieve_similar_cases(state: AgentState) -> Dict:
    """
    ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ë…¸ë“œ (RAG + íŒ¨í„´ ë§¤ì¹­)

    ë³‘ë ¬ ì²˜ë¦¬:
    - RAG ê²€ìƒ‰ (ChromaDB)
    - ì‹¤ì‹œê°„ íŒ¨í„´ ë¶„ì„ (JSON)

    Args:
        state: ì—ì´ì „íŠ¸ ìƒíƒœ

    Returns:
        ì—…ë°ì´íŠ¸ëœ ìƒíƒœ
    """
    print("\n" + "=" * 60)
    print("ğŸ“š [2/4] ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ë° íŒ¨í„´ ë§¤ì¹­ ì¤‘...")
    print("=" * 60)

    message = state["message"]
    sender = state.get("sender")

    print(f"  â†’ ê²€ìƒ‰ ì¿¼ë¦¬: {message[:50]}...")
    if sender:
        print(f"  â†’ ë°œì‹ ì: {sender}")

    with ThreadPoolExecutor(max_workers=2) as executor:
        # ragê²€ìƒ‰
        rag_future = executor.submit(search_vector_store, message, 5)
        # íŒ¨í„´ê²€ìƒ‰
        pattern_future = executor.submit(analyze_realtime_patterns, message, sender)

        # ê²°ê³¼ ìˆ˜ì§‘ (íƒ€ì„ì•„ì›ƒ 1ì´ˆ)
        try:
            rag_docs = rag_future.result(timeout=1.0)
        except Exception:
            print(f"  âš ï¸ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            rag_docs = []

        try:
            pattern_docs, pattern_analysis = pattern_future.result(timeout=2.0)
        except Exception:
            print(f"  âš ï¸ íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            pattern_docs, pattern_analysis = [], {}
    print(f"  â†’ RAG: {len(rag_docs)}ê°œ ìœ ì‚¬ ì‚¬ë¡€")
    print(f"  â†’ íŒ¨í„´: {len(pattern_docs)}ê°œ ë§¤ì¹­")

    # íŒ¨í„´ë¶„ì„ ê²°ê³¼ì¶œë ¥
    if pattern_analysis:
        risk = pattern_analysis.get("risk_summary", {})
        if level := risk.get("highest_level"):
            print(f"  â†’ ìœ„í—˜ë„: {level}")
        if matches := pattern_analysis.get("scam_matches"):
            print(f"  â†’ {len(matches)}ê°œ ì‚¬ê¸° ìœ í˜• ë§¤ì¹­")

    # ì „ì²´ ë¬¸ì„œ (RAG + íŒ¨í„´)
    all_similar_cases = rag_docs + pattern_docs

    # ìƒíƒœ ì—…ë°ì´íŠ¸
    return {
        "similar_cases": all_similar_cases,
        "matched_patterns": pattern_analysis.get("scam_matches", []),
    }
